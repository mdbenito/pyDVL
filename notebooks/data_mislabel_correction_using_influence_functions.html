<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Influence functions for data mislabeling &mdash; pyDVL 0.1.0.dev13 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="KNN Shapley" href="knn_shapley.html" />
    <link rel="prev" title="Notebooks" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> pyDVL
          </a>
              <div class="version">
                0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Guides and Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing pyDVL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Notebooks</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Influence functions for data mislabeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Plotting-the-dataset">Plotting the dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Calculating-influences-using-different-reduction-operators">Calculating influences using different reduction operators</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Flipping-5%-of-the-training-dataset-and-restore-almost-80%-of-the-flipped-samples">Flipping 5% of the training dataset and restore almost 80% of the flipped samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Appendix">Appendix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Appendix-A:-Calculating-the-decision-boundary">Appendix A: Calculating the decision boundary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Appendix-B:-Different-influence-aggregation-methods">Appendix B: Different influence aggregation methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="knn_shapley.html">KNN Shapley</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_networks_using_influence_functions.html">Clean dataset using influence functions and neural networks</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../valuation/index.html">valuation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pyDVL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Notebooks</a> &raquo;</li>
      <li>Influence functions for data mislabeling</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/data_mislabel_correction_using_influence_functions.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Influence-functions-for-data-mislabeling">
<h1>Influence functions for data mislabeling<a class="headerlink" href="#Influence-functions-for-data-mislabeling" title="Permalink to this heading"></a></h1>
<p>Data mislabeling occurs whenever some examples from a usually big dataset are wrongly-labeled. This hardly violates the assumption that the data distribution of the data equals the hidden distribution of the real world. Hence, it is important to be able to restore such datasets automatically. In real-life scenarios, this happens fairly often and can have various reasons, e.g. problems with identifiability, human error, or noise in the data. Imagine a simple classification problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
x_i &amp;\in \mathbb{R}^d \\
y_i &amp;\in \{0, 1\} \\
\forall i &amp;\in [ N ]
\end{align*}\end{split}\]</div>
<p>containing <span class="math notranslate nohighlight">\(N\)</span> samples. A classical example is whether a patient has a disease or not based on some feature representation <span class="math notranslate nohighlight">\(x\)</span> of that person. Furthermore, this formalism applies to all datasets, which can be transformed to a vector of size <span class="math notranslate nohighlight">\(d\)</span> by either linearization or other embedding techniques. For the described model the optimal decision boundary can be derived manually. This showed to be advantageous for educational purposes. Using a Bernoulli distribution on the classes
<span class="math notranslate nohighlight">\(y\)</span> and for the features <span class="math notranslate nohighlight">\(x\)</span> a Gaussian distribution conditioned on the previous samples class label <span class="math notranslate nohighlight">\(y\)</span>. This can be formalized as a graphical model</p>
<div class="math notranslate nohighlight">
\[\begin{split}y_i \sim \text{Ber}\left (0.5 \right) \\
x_i \sim \mathcal{N}\left ((1 - y_i) \mu_1 + y_i \mu_2, \sigma^2 I \right),\end{split}\]</div>
<p>with fixed means and diagonal covariance. Implementing the sampling scheme in python is straightforward and can be achieved by first sampling <span class="math notranslate nohighlight">\(y\)</span> and afterward <span class="math notranslate nohighlight">\(x\)</span>. More formally <span class="math notranslate nohighlight">\(x\)</span> is a function of <span class="math notranslate nohighlight">\(y\)</span> and not vice versa. The following code snippet</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">valuation.utils.numeric</span> <span class="kn">import</span> <span class="n">sample_classification_dataset_using_gaussians</span>

<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mus</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_classification_dataset_using_gaussians</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>generates the aforementioned dataset. After the dataset was generated, a closer inspection of the data is performed. The following code snippet explicitly calculates</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">valuation.utils.numeric</span> <span class="kn">import</span> <span class="n">decision_boundary_fixed_variance_2d</span>

<span class="n">decision_boundary_fn</span> <span class="o">=</span> <span class="n">decision_boundary_fixed_variance_2d</span><span class="p">(</span><span class="n">mus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mus</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">decision_boundary</span> <span class="o">=</span> <span class="n">decision_boundary_fn</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>the decision boundary by mapping a continuous line of z values to a 2-dimensional vector in feature space. For more information view appendix A.</p>
<section id="Plotting-the-dataset">
<h2>Plotting the dataset<a class="headerlink" href="#Plotting-the-dataset" title="Permalink to this heading"></a></h2>
<p>Next step consists in wrapping the previously generated data into a dataset with separate training and test set.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">valuation.utils</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">arg_flipper</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">:</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span> <span class="c1"># hacky</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="o">*</span><span class="n">arg_flipper</span><span class="p">(</span><span class="o">*</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.70</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<p>Subsequently, the data is plotted with their respective class labels 0 or 1.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">valuation.utils.plotting</span> <span class="kn">import</span> <span class="n">plot_datasets</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">y_train</span><span class="p">),</span>
    <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">x_test</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">y_test</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">x_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">plot_datasets</span><span class="p">(</span><span class="n">datasets</span><span class="p">,</span> <span class="n">x_min</span><span class="o">=</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="o">=</span><span class="n">x_max</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="n">decision_boundary</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_data_mislabel_correction_using_influence_functions_7_0.png" src="../_images/notebooks_data_mislabel_correction_using_influence_functions_7_0.png" />
</div>
</div>
<p>Note that both the train and test set are plotted side by side and the training samples overlap with the optimal decision boundary. These samples would get wrongly by any discriminator as this region has some identifiability issues.</p>
</section>
<section id="Calculating-influences-using-different-reduction-operators">
<h2>Calculating influences using different reduction operators<a class="headerlink" href="#Calculating-influences-using-different-reduction-operators" title="Permalink to this heading"></a></h2>
<p>This section cares about how to calculate influences for this dataset under the assumption of using a logistic regression model for inferring the right labels. Using the pyDVL valuation library a model can be formalized and fitted by using just a few lines of code.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">valuation.models</span> <span class="kn">import</span> <span class="n">TorchOptimizer</span><span class="p">,</span> <span class="n">TorchModule</span><span class="p">,</span> <span class="n">TorchObjective</span>
<span class="kn">from</span> <span class="nn">valuation.models</span> <span class="kn">import</span> <span class="n">BinaryLogisticRegressionTorchModel</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TorchModule</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">BinaryLogisticRegressionTorchModel</span><span class="p">(</span><span class="n">num_features</span><span class="p">),</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">TorchObjective</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">),</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">TorchOptimizer</span><span class="o">.</span><span class="n">ADAM_W</span><span class="p">,</span>
    <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.005</span><span class="p">,</span>
        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.005</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">y_train</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>It is important that the model converges to a point near the optimum, or otherwise the influence values will be of bad quality. Next the influences with respect to the previously fitted logistic regression model are calculated. A influence function</p>
<div class="math notranslate nohighlight">
\[I(x_1, y_1, x_2, y_2) \colon \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\]</div>
<p>measures the influence of the data point <span class="math notranslate nohighlight">\(x_1\)</span> onto <span class="math notranslate nohighlight">\(x_2\)</span> conditioned on the training targets <span class="math notranslate nohighlight">\(y_1\)</span> and <span class="math notranslate nohighlight">\(y_2\)</span> trough some model parameters <span class="math notranslate nohighlight">\(\theta\)</span>. As long as the loss function L is differentiable (or can be approximated by a surrogate objective) the influences</p>
<div class="math notranslate nohighlight">
\[I(x_1, x_2) = \nabla_\theta\; L(x_1, y_1) ^\mathsf{T} \; H_\theta^{-1} \; \nabla_\theta \; L(x_2, y_2)\]</div>
<p>can be linearly approximated. Using the pyDVL library the influences can be estimated by the following snippet</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">valuation.influence.general</span> <span class="kn">import</span> <span class="n">influences</span>
<span class="kn">from</span> <span class="nn">valuation.influence.types</span> <span class="kn">import</span> <span class="n">InfluenceTypes</span>
<span class="n">train_influences</span> <span class="o">=</span> <span class="n">influences</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">x_test</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span>
    <span class="n">influence_type</span><span class="o">=</span><span class="n">InfluenceTypes</span><span class="o">.</span><span class="n">Up</span>
<span class="p">)</span>
<span class="n">test_influences</span> <span class="o">=</span> <span class="n">influences</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">x_test</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span>
    <span class="n">influence_type</span><span class="o">=</span><span class="n">InfluenceTypes</span><span class="o">.</span><span class="n">Up</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>using autograd and explicit construction of the Hessian. Recall the train influences have shape [NxM] where N is the number of test samples and M is the number of training samples. The keene reader notices that, in order to obtain a valid ranking for the training data, each column of the aforementioned matrix has to be reduced to a single value, resulting overall in an vector of size [M]. There are various different choices in order to select See also appendix B. This notebook restricts to use
the mean absolute influence to filter for the wrong data labels. After calculating</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_influences</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">arr</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">arr</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean_train_influences</span> <span class="o">=</span> <span class="n">mean_influences</span><span class="p">(</span><span class="n">train_influences</span><span class="p">)</span>
<span class="n">mean_test_influences</span> <span class="o">=</span> <span class="n">mean_influences</span><span class="p">(</span><span class="n">test_influences</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>the data is again visualized along with their influences.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">influence_datasets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">mean_train_influences</span><span class="p">),</span>
    <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">x_test</span><span class="p">,</span> <span class="n">mean_test_influences</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">plot_datasets</span><span class="p">(</span><span class="n">influence_datasets</span><span class="p">,</span> <span class="n">x_min</span><span class="o">=</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="o">=</span><span class="n">x_max</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="n">decision_boundary</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_data_mislabel_correction_using_influence_functions_15_0.png" src="../_images/notebooks_data_mislabel_correction_using_influence_functions_15_0.png" />
</div>
</div>
</section>
<section id="Flipping-5%-of-the-training-dataset-and-restore-almost-80%-of-the-flipped-samples">
<h2>Flipping 5% of the training dataset and restore almost 80% of the flipped samples<a class="headerlink" href="#Flipping-5%-of-the-training-dataset-and-restore-almost-80%-of-the-flipped-samples" title="Permalink to this heading"></a></h2>
<p>It is assumed that our reference test set is not flipped and was checked. Usually this is a viable solution as the test set is much smaller than the train set. First 5% of the training set get flipped at random positions. Second it is shown how to identify these examples. So a flipped dataset is created by</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">valuation.utils.dataset</span> <span class="kn">import</span> <span class="n">flip_dataset</span>

<span class="n">flipped_dataset</span><span class="p">,</span> <span class="n">flipped_idx</span> <span class="o">=</span> <span class="n">flip_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">flip_percentage</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>sampling random indices and inverting those. It is noteworthy to say that a new model has to fitted, because otherwise the old model contains information about the correct label of the specific data samples. Hence,</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">valuation.models</span> <span class="kn">import</span> <span class="n">TorchModule</span><span class="p">,</span> <span class="n">TorchOptimizer</span><span class="p">,</span> <span class="n">TorchObjective</span>
<span class="kn">from</span> <span class="nn">valuation.models</span> <span class="kn">import</span> <span class="n">BinaryLogisticRegressionTorchModel</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">flipped_model</span> <span class="o">=</span> <span class="n">TorchModule</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">BinaryLogisticRegressionTorchModel</span><span class="p">(</span><span class="n">num_features</span><span class="p">),</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">TorchObjective</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">),</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">TorchOptimizer</span><span class="o">.</span><span class="n">ADAM_W</span><span class="p">,</span>
    <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.005</span><span class="p">,</span>
        <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.005</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">flipped_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">flipped_dataset</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
    <span class="n">flipped_dataset</span><span class="o">.</span><span class="n">y_train</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>a new model is fitted with the pyDVL library. The newly obtained model is then used along with the flipped dataset to obtain the influences. Recall the aforementioned influence metrics and more specifically the mean absolute influence. For each training sample this metric is calculated and the 5% data points with the highest associated metric are extracted.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">flipped_train_test_influences</span> <span class="o">=</span> <span class="n">influences</span><span class="p">(</span>
    <span class="n">flipped_model</span><span class="p">,</span>
    <span class="n">flipped_dataset</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
    <span class="n">flipped_dataset</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">flipped_dataset</span><span class="o">.</span><span class="n">x_test</span><span class="p">,</span>
    <span class="n">flipped_dataset</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span>
    <span class="n">influence_type</span><span class="o">=</span><span class="n">InfluenceTypes</span><span class="o">.</span><span class="n">Up</span>
<span class="p">)</span>
<span class="n">mean_flipped_train_test_influences</span> <span class="o">=</span> <span class="n">mean_influences</span><span class="p">(</span><span class="n">flipped_train_test_influences</span><span class="p">)</span>
<span class="n">estimated_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">mean_flipped_train_test_influences</span><span class="p">))[:</span><span class="nb">len</span><span class="p">(</span><span class="n">flipped_idx</span><span class="p">)]</span>
<span class="n">found_elements</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">estimated_idx</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">flipped_idx</span><span class="p">))</span>
<span class="n">remaining_element</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">flipped_idx</span><span class="p">)</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">estimated_idx</span><span class="p">))</span>
<span class="sa">f</span><span class="s2">&quot;Around </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">found_elements</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">flipped_idx</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% could be identified. But there are </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">remaining_element</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">flipped_idx</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% remaining samples&quot;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;Around 72.86% could be identified. But there are 27.14% remaining samples&#39;
</pre></div></div>
</div>
<p>Furthermore, the accuracy is evaluated, e.g. the number of same elements in the ground truth and the detected samples. Depending on the dataset a detection of up to 80% percent could be achieved. One might further inspect the selection method for the indices as it only selects the highest influence points as flipped samples. Furthermore, it is unclear how flipping all samples back and retraining the model affects the loss of the initial dataset.</p>
</section>
</section>
<section id="Appendix">
<h1>Appendix<a class="headerlink" href="#Appendix" title="Permalink to this heading"></a></h1>
<section id="Appendix-A:-Calculating-the-decision-boundary">
<h2>Appendix A: Calculating the decision boundary<a class="headerlink" href="#Appendix-A:-Calculating-the-decision-boundary" title="Permalink to this heading"></a></h2>
<p>For obtaining the optimal discriminator one has to solve the equation</p>
<div class="math notranslate nohighlight">
\[p(x|y=0)=p(x|y=1)\]</div>
<p>and determine the solution set <span class="math notranslate nohighlight">\(X\)</span>. A closed-form solution can be found, if the pdf of both classes can be written in closed form. In the general case, this decision boundary must not be linear. However, in the case of two Gaussians</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
p(x|y=0)&amp;=\mathcal{N}\left (\mu_1, \sigma^2 I \right) \\
p(x|y=1)&amp;=\mathcal{N}\left (\mu_2, \sigma^2 I \right)
\end{align*}\end{split}\]</div>
<p>the boundary is linear and is straightforward to derive it. The complete case with different full covariances is left to the reader as an exercise. For a single fixed diagonal variance parameterized by <span class="math notranslate nohighlight">\(\sigma\)</span> the equation can be directly rewritten as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\| x - \mu_1 \|^2 &amp;= \| x - \mu_2 \|^2 \\
\| \mu_1 \|^2 -2 x^\mathsf{T} \mu_1 &amp;= \| \mu_2 \|^2 -2 x^\mathsf{T} \mu_2 \\
\implies 0 &amp;= 2 (\mu_2 - \mu_1)^\mathsf{T} x + \| \mu_1 \|^2 - \| \mu_2 \|^2 \\
0 &amp;= \mu_1^\mathsf{T}x - \mu_2^\mathsf{T}x - \frac{1}{2} \mu_1^\mathsf{T} \mu_1 + \frac{1}{2} \mu_2^\mathsf{T} \mu_2
\end{align*}\end{split}\]</div>
<p>by using linear decision theory (see [Bishop C. 2006] for more details). However, this implicit description has to be transferred to an explicit one to effectively use it in python. Solving for the explicit form can be achieved by enforcing a functional form <span class="math notranslate nohighlight">\(f(z) = x = a z + b\)</span> with <span class="math notranslate nohighlight">\(z \in \mathbb{R}\)</span> onto <span class="math notranslate nohighlight">\(x\)</span>. After the term is inserted in the previous equation</p>
<div class="math notranslate nohighlight">
\[0 = (\mu_2 - \mu_1)^\mathsf{T} (az + b) + \frac{1}{2} \| \mu_1 \|^2 - \| \mu_2 \|^2\]</div>
<p>by setting <span class="math notranslate nohighlight">\(a\)</span> to be explicitly orthogonal to <span class="math notranslate nohighlight">\(\mu_2 - \mu_1\)</span> and then solving for <span class="math notranslate nohighlight">\(b\)</span> the solution</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(z) = \underbrace{\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix} (\mu_2 - \mu_1)}_a z + \underbrace{\frac{\mu_1 + \mu_2}{2}}_b\end{split}\]</div>
<p>for the functional form <span class="math notranslate nohighlight">\(f(z) = a z + b\)</span> can be obtained.</p>
</section>
<section id="Appendix-B:-Different-influence-aggregation-methods">
<h2>Appendix B: Different influence aggregation methods<a class="headerlink" href="#Appendix-B:-Different-influence-aggregation-methods" title="Permalink to this heading"></a></h2>
<p>A few strictly positive sample metrics</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\text{MAI}(x) &amp;= \frac{1}{N} \sum_{i=1}^N | I(x, x_i) | \\
\text{PMAI}(x) &amp;= \frac{1}{N} \sum_{i=1}^N \max(0, I(x, x_i))  \\
\text{NMAI}(x) &amp;= \frac{1}{N} \sum_{i=1}^N \max(0, -I(x, x_i))
\end{align*}\end{split}\]</div>
<p>are given. Note the functional relation <span class="math notranslate nohighlight">\(\text{MAI}(x) = \text{PMAI}(x) + \text{NMAI}(x)\)</span>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Notebooks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="knn_shapley.html" class="btn btn-neutral float-right" title="KNN Shapley" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>